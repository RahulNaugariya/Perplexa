{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayan112207/Perplexa/blob/main/Perplexa_Research_Mode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eueelBJDXVfN",
        "outputId": "9c342865-9e09-4f8c-edf2-649ed00627db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv==2.1.3\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting llama_index==0.12.3\n",
            "  Downloading llama_index-0.12.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-llms-mistralai==0.3.0\n",
            "  Downloading llama_index_llms_mistralai-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting llama-index-embeddings-mistralai==0.3.0\n",
            "  Downloading llama_index_embeddings_mistralai-0.3.0-py3-none-any.whl.metadata (696 bytes)\n",
            "Collecting gradio==3.39.0\n",
            "  Downloading gradio-3.39.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv==2.1.3)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv==2.1.3) (2.32.3)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama_index==0.12.3)\n",
            "  Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama_index==0.12.3)\n",
            "  Downloading llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.3 (from llama_index==0.12.3)\n",
            "  Downloading llama_index_core-0.12.23.post2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama_index==0.12.3)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama_index==0.12.3)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.8-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama_index==0.12.3)\n",
            "  Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama_index==0.12.3)\n",
            "  Downloading llama_index_llms_openai-0.3.25-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.4.0,>=0.3.0 (from llama_index==0.12.3)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.3.0-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama_index==0.12.3)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama_index==0.12.3)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama_index==0.12.3)\n",
            "  Downloading llama_index_readers_file-0.4.6-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama_index==0.12.3)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama_index==0.12.3) (3.9.1)\n",
            "Collecting mistralai>=1.0.0 (from llama-index-llms-mistralai==0.3.0)\n",
            "  Downloading mistralai-1.5.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==3.39.0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: aiohttp~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (3.11.13)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (5.5.0)\n",
            "Collecting fastapi (from gradio==3.39.0)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio==3.39.0)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client>=0.3.0 (from gradio==3.39.0)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (3.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.39.0) (3.0.0)\n",
            "Collecting markupsafe~=2.0 (from gradio==3.39.0)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (3.10.0)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.39.0)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (2.2.2)\n",
            "Collecting pillow<11.0,>=8.0 (from gradio==3.39.0)\n",
            "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (2.10.6)\n",
            "Collecting pydub (from gradio==3.39.0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart (from gradio==3.39.0)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (6.0.2)\n",
            "Collecting semantic-version~=2.0 (from gradio==3.39.0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==3.39.0) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio==3.39.0)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio==3.39.0)\n",
            "  Downloading websockets-11.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp~=3.0->gradio==3.39.0) (1.18.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==3.39.0) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==3.39.0) (1.29.0)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv==2.1.3)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client>=0.3.0->gradio==3.39.0) (2024.10.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->gradio==3.39.0) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->gradio==3.39.0) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->gradio==3.39.0) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->gradio==3.39.0) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->gradio==3.39.0) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.14.0->gradio==3.39.0) (3.17.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.14.0->gradio==3.39.0) (4.67.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama_index==0.12.3) (1.61.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.3->llama_index==0.12.3) (2.0.38)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.3->llama_index==0.12.3)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.3->llama_index==0.12.3) (1.2.18)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.3->llama_index==0.12.3)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.3->llama_index==0.12.3)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.3->llama_index==0.12.3) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.3->llama_index==0.12.3) (3.4.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.3->llama_index==0.12.3) (9.0.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.3->llama_index==0.12.3)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.3->llama_index==0.12.3)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.3->llama_index==0.12.3) (1.17.2)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama_index==0.12.3)\n",
            "  Downloading llama_cloud-0.1.14-py3-none-any.whl.metadata (902 bytes)\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.2.0 (from llama-index-core<0.13.0,>=0.12.3->llama_index==0.12.3)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index==0.12.3) (4.13.3)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index==0.12.3)\n",
            "  Downloading pypdf-5.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index==0.12.3)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index==0.12.3)\n",
            "  Downloading llama_parse-0.6.4.post1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.0.0->markdown-it-py[linkify]>=2.0.0->gradio==3.39.0) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.39.0) (2.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==3.39.0) (2.8.2)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio==3.39.0)\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "INFO: pip is still looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio==3.39.0)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting eval-type-backport>=0.2.0 (from mistralai>=1.0.0->llama-index-llms-mistralai==0.3.0)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting jsonpath-python>=1.0.6 (from mistralai>=1.0.0->llama-index-llms-mistralai==0.3.0)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index==0.12.3) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index==0.12.3) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index==0.12.3) (2024.11.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==3.39.0) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==3.39.0) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.39.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.39.0) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv==2.1.3) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv==2.1.3) (2.3.0)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi->gradio==3.39.0)\n",
            "  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama_index==0.12.3) (2.6)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.39.0) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.39.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.39.0) (0.23.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.39.0) (1.0.3)\n",
            "Collecting llama-cloud-services>=0.6.4 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index==0.12.3)\n",
            "  Downloading llama_cloud_services-0.6.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama_index==0.12.3) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama_index==0.12.3) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama_index==0.12.3) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.39.0) (1.17.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.3->llama_index==0.12.3) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.3->llama_index==0.12.3)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.3->llama_index==0.12.3)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.4->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index==0.12.3)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index-0.12.3-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_llms_mistralai-0.3.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_embeddings_mistralai-0.3.0-py3-none-any.whl (2.6 kB)\n",
            "Downloading gradio-3.39.0-py3-none-any.whl (19.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_agent_openai-0.4.6-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.1-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.12.23.post2-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.8-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.3.25-py3-none-any.whl (16 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.3.0-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.6-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading mistralai-1.5.1-py3-none-any.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-11.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading llama_cloud-0.1.14-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.7/261.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.4.post1-py3-none-any.whl (4.9 kB)\n",
            "Downloading pypdf-5.3.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading llama_cloud_services-0.6.5-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=3fd8361b5bb6119390335d0f3bd511719453f69c25470d42934b7e5b6d982e2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: striprtf, sgmllib3k, pydub, filetype, dirtyjson, websockets, uvicorn, tenacity, semantic-version, python-multipart, python-dotenv, pypdf, pillow, mypy-extensions, marshmallow, markupsafe, markdown-it-py, jsonpath-python, ffmpy, feedparser, eval-type-backport, aiofiles, typing-inspect, tiktoken, starlette, mdit-py-plugins, arxiv, mistralai, llama-cloud, gradio-client, fastapi, dataclasses-json, llama-index-legacy, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-llms-mistralai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-embeddings-mistralai, llama-cloud-services, gradio, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 14.2\n",
            "    Uninstalling websockets-14.2:\n",
            "      Successfully uninstalled websockets-14.2\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: mdit-py-plugins\n",
            "    Found existing installation: mdit-py-plugins 0.4.2\n",
            "    Uninstalling mdit-py-plugins-0.4.2:\n",
            "      Successfully uninstalled mdit-py-plugins-0.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.2.0 requires websockets<15.0dev,>=13.0, but you have websockets 11.0.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 arxiv-2.1.3 dataclasses-json-0.6.7 dirtyjson-1.0.8 eval-type-backport-0.2.2 fastapi-0.115.11 feedparser-6.0.11 ffmpy-0.5.0 filetype-1.2.0 gradio-3.39.0 gradio-client-1.7.2 jsonpath-python-1.0.6 llama-cloud-0.1.14 llama-cloud-services-0.6.5 llama-index-agent-openai-0.4.6 llama-index-cli-0.4.1 llama-index-core-0.12.23.post2 llama-index-embeddings-mistralai-0.3.0 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.8 llama-index-legacy-0.9.48.post4 llama-index-llms-mistralai-0.3.0 llama-index-llms-openai-0.3.25 llama-index-multi-modal-llms-openai-0.3.0 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.6 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.4.post1 llama_index-0.12.3 markdown-it-py-2.2.0 markupsafe-2.1.5 marshmallow-3.26.1 mdit-py-plugins-0.3.3 mistralai-1.5.1 mypy-extensions-1.0.0 pillow-10.4.0 pydub-0.25.1 pypdf-5.3.1 python-dotenv-1.0.1 python-multipart-0.0.20 semantic-version-2.10.0 sgmllib3k-1.0.0 starlette-0.46.0 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.9.0 typing-inspect-0.9.0 uvicorn-0.34.0 websockets-11.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "1b3910b228784d8f8f97f958b480b37f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install arxiv==2.1.3 llama_index==0.12.3 llama-index-llms-mistralai==0.3.0 llama-index-embeddings-mistralai==0.3.0 gradio==3.39.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8gYE1HnNXpEH"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "import requests\n",
        "import sys\n",
        "import arxiv\n",
        "from llama_index.llms.mistralai import MistralAI\n",
        "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, StorageContext, load_index_from_storage, PromptTemplate, Settings\n",
        "from llama_index.core.tools import FunctionTool, QueryEngineTool\n",
        "from llama_index.core.agent import ReActAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Wj9TcnLGYFHb"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('MISTRAL_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y7raylcnYNN6"
      },
      "outputs": [],
      "source": [
        "llm = MistralAI(api_key=api_key, model='mistral-large-latest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "spQgVerwYSBc"
      },
      "outputs": [],
      "source": [
        "model_name = \"mistral-embed\"\n",
        "embed_model = MistralAIEmbedding(model_name=model_name, api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DuWmybEZYVFx"
      },
      "outputs": [],
      "source": [
        "def fetch_arxiv_papers(title :str, papers_count: int):\n",
        "    search_query = f'all:\"{title}\"'\n",
        "    search = arxiv.Search(\n",
        "        query=search_query,\n",
        "        max_results=papers_count,\n",
        "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
        "        sort_order=arxiv.SortOrder.Descending\n",
        "    )\n",
        "\n",
        "    papers = []\n",
        "    # Use the Client for searching\n",
        "    client = arxiv.Client()\n",
        "\n",
        "    # Execute the search\n",
        "    search = client.results(search)\n",
        "\n",
        "    for result in search:\n",
        "        paper_info = {\n",
        "                'title': result.title,\n",
        "                'authors': [author.name for author in result.authors],\n",
        "                'summary': result.summary,\n",
        "                'published': result.published,\n",
        "                'journal_ref': result.journal_ref,\n",
        "                'doi': result.doi,\n",
        "                'primary_category': result.primary_category,\n",
        "                'categories': result.categories,\n",
        "                'pdf_url': result.pdf_url,\n",
        "                'arxiv_url': result.entry_id\n",
        "            }\n",
        "        papers.append(paper_info)\n",
        "\n",
        "    return papers\n",
        "\n",
        "papers = fetch_arxiv_papers(\"Language Models\", 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UavdxwmjYYV1",
        "outputId": "e9d87dc9-4250-4b95-ce00-9fec013c9678"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling'],\n",
              " ['LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM'],\n",
              " ['Shifting Long-Context LLMs Research from Input to Output'],\n",
              " ['Enough Coin Flips Can Make LLMs Act Bayesian'],\n",
              " ['Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities'],\n",
              " ['Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining'],\n",
              " ['Scaling Rich Style-Prompted Text-to-Speech Datasets'],\n",
              " ['Universality of Layer-Level Entropy-Weighted Quantization Beyond Model Architecture and Size'],\n",
              " ['L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning'],\n",
              " ['UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets']]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "[[p['title']] for p in papers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kCiVIJuqYbCI"
      },
      "outputs": [],
      "source": [
        "def create_documents_from_papers(papers):\n",
        "    documents = []\n",
        "    for paper in papers:\n",
        "        content = f\"Title: {paper['title']}\\n\" \\\n",
        "                  f\"Authors: {', '.join(paper['authors'])}\\n\" \\\n",
        "                  f\"Summary: {paper['summary']}\\n\" \\\n",
        "                  f\"Published: {paper['published']}\\n\" \\\n",
        "                  f\"Journal Reference: {paper['journal_ref']}\\n\" \\\n",
        "                  f\"DOI: {paper['doi']}\\n\" \\\n",
        "                  f\"Primary Category: {paper['primary_category']}\\n\" \\\n",
        "                  f\"Categories: {', '.join(paper['categories'])}\\n\" \\\n",
        "                  f\"PDF URL: {paper['pdf_url']}\\n\" \\\n",
        "                  f\"arXiv URL: {paper['arxiv_url']}\\n\"\n",
        "        documents.append(Document(text=content))\n",
        "    return documents\n",
        "\n",
        "\n",
        "\n",
        "#Create documents for LlamaIndex\n",
        "documents = create_documents_from_papers(papers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "E-j_Mbh3YeMI"
      },
      "outputs": [],
      "source": [
        "Settings.chunk_size = 1024\n",
        "Settings.chunk_overlap = 50\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kEVGvAG9YgCo"
      },
      "outputs": [],
      "source": [
        "index.storage_context.persist('index/')\n",
        "# rebuild storage context\n",
        "storage_context = StorageContext.from_defaults(persist_dir='index/')\n",
        "\n",
        "#load index\n",
        "index = load_index_from_storage(storage_context, embed_model=embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kvaUsTCkYif6"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine(llm=llm, similarity_top_k=5)\n",
        "\n",
        "rag_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine,\n",
        "    name=\"research_paper_query_engine_tool\",\n",
        "    description=\"A RAG engine with recent research papers.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "j5vwUeC-Yk_A",
        "outputId": "5d1f1898-af6d-48bd-8bca-f7bcc19b9fd4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Prompt Key**: response_synthesizer:text_qa_template**Text:** "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context information is below.\n",
            "---------------------\n",
            "{context_str}\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: {query_str}\n",
            "Answer: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ""
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Prompt Key**: response_synthesizer:refine_template**Text:** "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original query is as follows: {query_str}\n",
            "We have provided an existing answer: {existing_answer}\n",
            "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
            "------------\n",
            "{context_msg}\n",
            "------------\n",
            "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
            "Refined Answer: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ""
          },
          "metadata": {}
        }
      ],
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "from IPython.display import Markdown, display\n",
        "# define prompt viewing function\n",
        "def display_prompt_dict(prompts_dict):\n",
        "    for k, p in prompts_dict.items():\n",
        "        text_md = f\"**Prompt Key**: {k}\" f\"**Text:** \"\n",
        "        display(Markdown(text_md))\n",
        "        print(p.get_template())\n",
        "        display(Markdown(\"\"))\n",
        "\n",
        "prompts_dict = query_engine.get_prompts()\n",
        "display_prompt_dict(prompts_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qGgcen-TYnZx"
      },
      "outputs": [],
      "source": [
        "def download_pdf(pdf_url, output_file):\n",
        "    \"\"\"\n",
        "    Downloads a PDF file from the given URL and saves it to the specified file.\n",
        "\n",
        "    Args:\n",
        "        pdf_url (str): The URL of the PDF file to download.\n",
        "        output_file (str): The path and name of the file to save the PDF to.\n",
        "\n",
        "    Returns:\n",
        "        str: A message indicating success or the nature of an error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Send a GET request to the PDF URL\n",
        "        response = requests.get(pdf_url)\n",
        "        response.raise_for_status()  # Raise an error for HTTP issues\n",
        "\n",
        "        # Write the content of the PDF to the output file\n",
        "        with open(output_file, \"wb\") as file:\n",
        "            file.write(response.content)\n",
        "\n",
        "        return f\"PDF downloaded successfully and saved as '{output_file}'.\"\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"An error occurred: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6dLs6R3xYt4K"
      },
      "outputs": [],
      "source": [
        "download_pdf_tool = FunctionTool.from_defaults(\n",
        "    download_pdf,\n",
        "    name='download_pdf_file_tool',\n",
        "    description='python function, which downloads a pdf file by link'\n",
        ")\n",
        "fetch_arxiv_tool = FunctionTool.from_defaults(\n",
        "    fetch_arxiv_papers,\n",
        "    name='fetch_from_arxiv',\n",
        "    description='download the {max_results} recent papers regarding the topic {title} from arxiv'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1BewGlOZYvor"
      },
      "outputs": [],
      "source": [
        "# building an ReAct Agent with the three tools.\n",
        "agent = ReActAgent.from_tools([download_pdf_tool, rag_tool, fetch_arxiv_tool], llm=llm, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "72T0WLIPYxPq"
      },
      "outputs": [],
      "source": [
        "# create a prompt template to chat with an agent\n",
        "q_template = (\n",
        "    \"I am interested in {topic}. \\n\"\n",
        "    \"Find papers in your knowledge database related to this topic; use the following template to query research_paper_query_engine_tool tool: 'Provide title, summary, authors and link to download for papers related to {topic}'. If there are not, could you fetch the recent one from arXiv? \\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_EKn9f5Yyxg",
        "outputId": "d0ccc5bc-fc90-450e-a2b6-722c9eb3be7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step be17b528-7d8c-4ee0-a2a7-5e7e4e494fb7. Step input: I am interested in Audio-Language Models. \n",
            "Find papers in your knowledge database related to this topic; use the following template to query research_paper_query_engine_tool tool: 'Provide title, summary, authors and link to download for papers related to Audio-Language Models'. If there are not, could you fetch the recent one from arXiv? \n",
            "\n",
            "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
            "Action: research_paper_query_engine_tool\n",
            "Action Input: {'input': 'Provide title, summary, authors and link to download for papers related to Audio-Language Models'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: The title of the paper is \"LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM.\" The authors are Sambal Shikhar, Mohammed Irfan Kurpath, Sahal Shaji Mullappilly, Jean Lahoud, Fahad Khan, Rao Muhammad Anwer, Salman Khan, and Hisham Cholakkal.\n",
            "\n",
            "Summary:\n",
            "This paper introduces LLMVoX, a lightweight, LLM-agnostic, autoregressive streaming Text-to-Speech (TTS) system designed to generate high-quality speech with low latency while preserving the base LLM's capabilities. LLMVoX addresses issues like fine-tuning requirements and text-speech misalignment, achieving a lower Word Error Rate and supporting seamless, infinite-length dialogues. Its plug-and-play design allows for easy extension to various tasks and languages, and it has been integrated with a Vision-Language Model to create an omni-model with speech, text, and vision capabilities.\n",
            "\n",
            "You can download the paper here: http://arxiv.org/pdf/2503.04724v1\n",
            "\u001b[0m> Running step aa24ba45-0ba7-4d79-b1d1-54b8a3b9af35. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
            "Answer: I found a paper related to Audio-Language Models. Here are the details:\n",
            "\n",
            "Title: LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM\n",
            "\n",
            "Authors: Sambal Shikhar, Mohammed Irfan Kurpath, Sahal Shaji Mullappilly, Jean Lahoud, Fahad Khan, Rao Muhammad Anwer, Salman Khan, Hisham Cholakkal\n",
            "\n",
            "Summary: This paper introduces LLMVoX, a lightweight, LLM-agnostic, autoregressive streaming Text-to-Speech (TTS) system designed to generate high-quality speech with low latency while preserving the base LLM's capabilities. LLMVoX addresses issues like fine-tuning requirements and text-speech misalignment, achieving a lower Word Error Rate and supporting seamless, infinite-length dialogues. Its plug-and-play design allows for easy extension to various tasks and languages, and it has been integrated with a Vision-Language Model to create an omni-model with speech, text, and vision capabilities.\n",
            "\n",
            "You can download the paper here: http://arxiv.org/pdf/2503.04724v1\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "answer = agent.chat(q_template.format(topic=\"Audio-Language Models\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "q6VPtnQkY4Fl",
        "outputId": "d1ca3f6c-b8b3-4d1a-d35a-1554260307ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I found a paper related to Audio-Language Models. Here are the details:\n\nTitle: LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM\n\nAuthors: Sambal Shikhar, Mohammed Irfan Kurpath, Sahal Shaji Mullappilly, Jean Lahoud, Fahad Khan, Rao Muhammad Anwer, Salman Khan, Hisham Cholakkal\n\nSummary: This paper introduces LLMVoX, a lightweight, LLM-agnostic, autoregressive streaming Text-to-Speech (TTS) system designed to generate high-quality speech with low latency while preserving the base LLM's capabilities. LLMVoX addresses issues like fine-tuning requirements and text-speech misalignment, achieving a lower Word Error Rate and supporting seamless, infinite-length dialogues. Its plug-and-play design allows for easy extension to various tasks and languages, and it has been integrated with a Vision-Language Model to create an omni-model with speech, text, and vision capabilities.\n\nYou can download the paper here: http://arxiv.org/pdf/2503.04724v1"
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "Markdown(answer.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNd4BbHXZuED",
        "outputId": "6e8ffb0e-63e5-4b50-f194-fb08849b25a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step 1a6d442f-407b-416a-8069-dca138f09ead. Step input: Download the papers, which you mentioned above\n",
            "\u001b[1;3;38;5;200mThought: I need to use a tool to help me answer the question.\n",
            "Action: download_pdf_file_tool\n",
            "Action Input: {'pdf_url': 'http://arxiv.org/pdf/2503.04724v1', 'output_file': 'LLMVoX_Autoregressive_Streaming_Text_to_Speech_Model_for_Any_LLM.pdf'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: PDF downloaded successfully and saved as 'LLMVoX_Autoregressive_Streaming_Text_to_Speech_Model_for_Any_LLM.pdf'.\n",
            "\u001b[0m> Running step 3fc5a9ae-0c0c-4d41-9bd3-e2de42fe9d41. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
            "Answer: The paper \"LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM\" has been downloaded successfully and saved as 'LLMVoX_Autoregressive_Streaming_Text_to_Speech_Model_for_Any_LLM.pdf'.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "answer = agent.chat(\"Download the papers, which you mentioned above\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "9YdelVk7ZzcR",
        "outputId": "02f894d6-66c0-483a-abb5-c948fad7eb97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper \"LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM\" has been downloaded successfully and saved as 'LLMVoX_Autoregressive_Streaming_Text_to_Speech_Model_for_Any_LLM.pdf'."
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "Markdown(answer.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiLhqJL-Z69x",
        "outputId": "cf2f6260-4bc4-4351-9cfe-1fce28800a97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step d05aa99e-d676-4119-82d4-153d0091b374. Step input: I am interested in Min Max Similarity. \n",
            "Find papers in your knowledge database related to this topic; use the following template to query research_paper_query_engine_tool tool: 'Provide title, summary, authors and link to download for papers related to Min Max Similarity'. If there are not, could you fetch the recent one from arXiv? \n",
            "\n",
            "\u001b[1;3;38;5;200mThought: I need to use a tool to help me answer the question.\n",
            "Action: research_paper_query_engine_tool\n",
            "Action Input: {'input': 'Provide title, summary, authors and link to download for papers related to Min Max Similarity'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: I'm sorry, but there are no papers related to \"Min Max Similarity\" in the given context information.\n",
            "\u001b[0m> Running step 863602f1-f120-494a-9665-c15b91099862. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I need to use a tool to help me answer the question.\n",
            "Action: fetch_from_arxiv\n",
            "Action Input: {'title': 'Min Max Similarity', 'papers_count': 1}\n",
            "\u001b[0m\u001b[1;3;34mObservation: [{'title': 'Channel Charting for Streaming CSI Data', 'authors': ['Sueda Taner', 'Maxime Guillaud', 'Olav Tirkkonen', 'Christoph Studer'], 'summary': 'Channel charting (CC) applies dimensionality reduction to channel state\\ninformation (CSI) data at the infrastructure basestation side with the goal of\\nextracting pseudo-position information for each user. The self-supervised\\nnature of CC enables predictive tasks that depend on user position without\\nrequiring any ground-truth position information. In this work, we focus on the\\npractically relevant streaming CSI data scenario, in which CSI is constantly\\nestimated. To deal with storage limitations, we develop a novel streaming CC\\narchitecture that maintains a small core CSI dataset from which the channel\\ncharts are learned. Curation of the core CSI dataset is achieved using a\\nmin-max-similarity criterion. Numerical validation with measured CSI data\\ndemonstrates that our method approaches the accuracy obtained from the complete\\nCSI dataset while using only a fraction of CSI storage and avoiding\\ncatastrophic forgetting of old CSI data.', 'published': datetime.datetime(2023, 12, 7, 18, 34, 25, tzinfo=datetime.timezone.utc), 'journal_ref': None, 'doi': None, 'primary_category': 'cs.IT', 'categories': ['cs.IT', 'eess.SP', 'math.IT'], 'pdf_url': 'http://arxiv.org/pdf/2312.04514v1', 'arxiv_url': 'http://arxiv.org/abs/2312.04514v1'}]\n",
            "\u001b[0m> Running step 8a533c40-4fab-4660-8f79-e8bb39631213. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
            "Answer: I found a recent paper related to Min Max Similarity from arXiv. Here are the details:\n",
            "\n",
            "Title: Channel Charting for Streaming CSI Data\n",
            "\n",
            "Authors: Sueda Taner, Maxime Guillaud, Olav Tirkkonen, Christoph Studer\n",
            "\n",
            "Summary: This paper introduces a novel streaming channel charting (CC) architecture that maintains a small core CSI dataset for learning channel charts. The method uses a min-max-similarity criterion for curating the core CSI dataset, allowing it to approach the accuracy of the complete CSI dataset while using only a fraction of the storage and avoiding catastrophic forgetting of old CSI data. The self-supervised nature of CC enables predictive tasks that depend on user position without requiring any ground-truth position information.\n",
            "\n",
            "You can download the paper here: http://arxiv.org/pdf/2312.04514v1\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "answer = agent.chat(q_template.format(topic=\"Min Max Similarity\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "porCL-XqaIwM",
        "outputId": "9ba562f1-a017-4dc2-8f86-5e05b1995f9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I found a recent paper related to Min Max Similarity from arXiv. Here are the details:\n\nTitle: Channel Charting for Streaming CSI Data\n\nAuthors: Sueda Taner, Maxime Guillaud, Olav Tirkkonen, Christoph Studer\n\nSummary: This paper introduces a novel streaming channel charting (CC) architecture that maintains a small core CSI dataset for learning channel charts. The method uses a min-max-similarity criterion for curating the core CSI dataset, allowing it to approach the accuracy of the complete CSI dataset while using only a fraction of the storage and avoiding catastrophic forgetting of old CSI data. The self-supervised nature of CC enables predictive tasks that depend on user position without requiring any ground-truth position information.\n\nYou can download the paper here: http://arxiv.org/pdf/2312.04514v1"
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "Markdown(answer.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SkQSUKFLbcba"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def research_agent(topic):\n",
        "    \"\"\"\n",
        "    Function to handle user queries, interact with the agent,\n",
        "    and return the agent's response.\n",
        "\n",
        "    Args:\n",
        "        topic (str): The user's research topic.\n",
        "\n",
        "    Returns:\n",
        "        str: The agent's response.\n",
        "    \"\"\"\n",
        "\n",
        "    answer = agent.chat(q_template.format(topic=topic))  # Get the agent's response\n",
        "    return (answer.response)  # Return the response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkiWA7V1b5P9",
        "outputId": "7bf3f0e4-9852-42b9-9c2b-189de95fd3da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-308c233bc927>:3: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  inputs=gr.inputs.Textbox(lines=2, placeholder=\"Enter your research topic here...\"),  # Input field\n",
            "<ipython-input-26-308c233bc927>:3: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  inputs=gr.inputs.Textbox(lines=2, placeholder=\"Enter your research topic here...\"),  # Input field\n",
            "<ipython-input-26-308c233bc927>:3: GradioDeprecationWarning: `numeric` parameter is deprecated, and it has no effect\n",
            "  inputs=gr.inputs.Textbox(lines=2, placeholder=\"Enter your research topic here...\"),  # Input field\n"
          ]
        }
      ],
      "source": [
        "iface = gr.Interface(\n",
        "    fn=research_agent,  # The function to handle queries\n",
        "    inputs=gr.inputs.Textbox(lines=2, placeholder=\"Enter your research topic here...\"),  # Input field\n",
        "    outputs=\"text\",  # Output format\n",
        "    title=\"Research Paper Agent\",  # Title of the interface\n",
        "    description=\"Explore recent research papers on various topics.\",  # Description\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "id": "HPKQ9xNib60x",
        "outputId": "aa1dd85c-6811-445a-b059-0db8b441c00f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://67848ace2fb4d16066.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://67848ace2fb4d16066.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step a702e91b-5b91-466a-afa2-9e8a56a3757f. Step input: I am interested in Large Language Models. \n",
            "Find papers in your knowledge database related to this topic; use the following template to query research_paper_query_engine_tool tool: 'Provide title, summary, authors and link to download for papers related to Large Language Models'. If there are not, could you fetch the recent one from arXiv? \n",
            "\n",
            "\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
            "Answer: I found several papers related to Large Language Models. Here are the details:\n",
            "\n",
            "1. **Title:** Shifting Long-Context LLMs Research from Input to Output\n",
            "   **Authors:** Yuhao Wu, Yushi Bai, Zhiqing Hu, Shangqing Tu, Ming Shan Hee, Juanzi Li, Roy Ka-Wei Lee\n",
            "   **Summary:** This paper advocates for a shift in NLP research towards addressing the challenges of long-output generation. Tasks like novel writing, long-term planning, and complex reasoning require models to produce coherent, contextually rich, and logically consistent extended text. The authors highlight the importance of developing foundational LLMs tailored for generating high-quality, long-form outputs.\n",
            "   **Download Link:** [PDF](http://arxiv.org/pdf/2503.04723v1)\n",
            "\n",
            "2. **Title:** Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining\n",
            "   **Authors:** Houyi Li, Wenzheng Zheng, Jingcheng Hu, Qiufeng Wang, Hanshan Zhang, Zili Wang, Yangshijie Xu, Shuigeng Zhou, Xiangyu Zhang, Daxin Jiang\n",
            "   **Summary:** This paper discusses the importance of hyperparameter optimization in Large Language Models. It presents universal scaling laws for optimal learning rate and batch size, contributing a plug-and-play tool for optimal hyperparameter estimation.\n",
            "   **Download Link:** [PDF](http://arxiv.org/pdf/2503.04715v1)\n",
            "\n",
            "3. **Title:** L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling\n",
            "   **Authors:** Zhuo Chen, Oriol Mayné i Comas, Zhuotao Jin, Di Luo, Marin Soljačić\n",
            "   **Summary:** This paper establishes a bipartite mutual information scaling law governing long-range dependencies in language. It introduces the L$^2$M condition, relating a model's capacity for long context lengths to its latent state size, and demonstrates its effectiveness in predicting long-range dependencies.\n",
            "   **Download Link:** [PDF](http://arx\n",
            "\u001b[0mKeyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://67848ace2fb4d16066.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "iface.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7SEEQHlS-uOl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKBs3IfoMMLSZU9O1tpIkp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}